{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Parallel transformers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfa825f8ed593e9b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23c60c6e40c36d24"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7140508473463848"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('~/Projects/transformer_wrappers/src')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "671b233e2a64bd30"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e75516abc65dc1c3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformer_wrappers.wrappers import ParallelCausalLMWrapper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d1f4c28581c5a2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants and globals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb2fdaeeea9156ae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TOKEN = None  # TODO: HF token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18e7a5f98a8b71f5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL = 'meta-llama/Meta-Llama-3-8B'  #  google/gemma-7b | mistralai/Mistral-7B-v0.3 | meta-llama/Meta-Llama-3-8B  \n",
    "FINE_TUNED_MODEL = './resources/models/fine_tuned/...'  # TODO: fine-tuned model path\n",
    "MODEL_CONFIGS = {\n",
    "    'token': TOKEN,\n",
    "    'device_map': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'load_in_4bit': True, \n",
    "    'bnb_4bit_use_double_quant': True, \n",
    "    'bnb_4bit_quant_type': 'nf4', \n",
    "    'bnb_4bit_compute_dtype': torch.bfloat16\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3c26d02f1e24b23"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "TOKENIZER = MODEL  # Make sure to match it with base model when using fine-tuned models\n",
    "TOKENIZER_CONFIGS = {\n",
    "    'token': TOKEN,\n",
    "    'pad_token': '<|end_of_text|>'  #  mistralai/Mistral-7B-v0.3 -> '</s>' | meta-llama/Meta-Llama-3-8B -> '<|end_of_text|>'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61a9585db4e30fa2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PARALLELISATION_CONFIG_KEYS_MAPPING = {\n",
    "    'p_rate': 'Parallelisation rate', \n",
    "    'block_parallel': 'Layer level parallelisation', \n",
    "    'iterative': 'Iterative', \n",
    "    'scaling': 'Normalisation'\n",
    "}\n",
    "PARALLELISATION_CONFIGS = [\n",
    "    {k: v for k, v in zip(WRAPPER_CONFIGS_KEYS, configs)}\n",
    "    for configs in itertools.product([2, 4], [True, False], [True, False], [True, False])\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f78610dd174f2a7f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PROMPT = 'Q: What is the capital of Italy? A:'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f34b6aef79ae744c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parallelise pre-trained model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c3dd90d6e25d58bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load pre-trained model into parallel wrapper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82dfcd795bbebc1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = ParallelCausalLMWrapper.from_pretrained(\n",
    "    MODEL, \n",
    "    model_kwargs=MODEL_CONFIGS,\n",
    "    tokenizer_kwargs=TOKENIZER_CONFIGS\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a76184817b80a2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Iterate through configs, generate from the given prompt and print generated output for each config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a26526c7a4f11244"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print('#' * 32)\n",
    "for configs in PARALLELISATION_CONFIGS:\n",
    "    print('\\n'.join(f'{PARALLELISATION_CONFIG_KEYS_MAPPING[k]}: {v}' for k, v in config.items()))\n",
    "    print('-' * 32)\n",
    "    input_encodings = model.tokenizer(PROMPT, return_tensors='pt').to(DEVICE)\n",
    "    output = model.generate(input_encodings.input_ids, **configs, do_sample=False, max_length=32)\n",
    "    text = model.tokenizer.decode(output['input_ids'][0])\n",
    "    print(repr(text))\n",
    "    print('#' * 32)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fcc5be874be4934"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuned parallelised model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46e576d87187b15c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load fine-tuned model into parallel wrapper"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a8386f7c2f3f07c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = ParallelCausalLMWrapper.from_pretrained(\n",
    "    FINE_TUNED_MODEL, \n",
    "    model_kwargs=MODEL_CONFIGS,\n",
    "    peft=True,\n",
    "    tokenizer_name_or_path=TOKENIZER,\n",
    "    tokenizer_kwargs=TOKENIZER_CONFIGS\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7befb6b90ab708a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate from the given prompt and print generated output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7a4b991eaddd4c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_encodings = model.tokenizer(PROMPT, return_tensors='pt').to(DEVICE)\n",
    "output = model.generate(input_encodings.input_ids, do_sample=False, max_length=32)\n",
    "text = model.tokenizer.decode(output['input_ids'][0])\n",
    "print(repr(text))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bee7a27e9da0be32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
